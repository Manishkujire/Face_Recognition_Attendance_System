{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f75b754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import OpenCV module\n",
    "import cv2\n",
    "#import os module for reading training data directories and paths\n",
    "import os\n",
    "#import numpy to convert python lists to numpy arrays as \n",
    "#it is needed by OpenCV face recognizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "from PIL import Image\n",
    "from numpy import * \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe474b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data_folder_path):\n",
    "    \n",
    "    #------STEP-1--------\n",
    "    #get the directories (one directory for each subject) in data folder\n",
    "    dirs = os.listdir(data_folder_path)\n",
    "    \n",
    "    #list to hold all subject faces\n",
    "    faces = []\n",
    "    #list to hold labels for all subjects\n",
    "    labels = []\n",
    "    \n",
    "    #let's go through each directory and read images within it\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        #our subject directories start with letter 's' so\n",
    "        #ignore any non-relevant directories if any\n",
    "        if not dir_name.startswith(\"s\"):\n",
    "            continue;\n",
    "            \n",
    "        #------STEP-2--------\n",
    "        #extract label number of subject from dir_name\n",
    "        #format of dir name = slabel\n",
    "        #, so removing letter 's' from dir_name will give us label\n",
    "        label = int(dir_name.replace(\"s\", \"\"))\n",
    "        \n",
    "        #build path of directory containin images for current subject subject\n",
    "        #sample subject_dir_path = \"training-data/s1\"\n",
    "        subject_dir_path = data_folder_path + \"/\" + dir_name\n",
    "        \n",
    "        #get the images names that are inside the given subject directory\n",
    "        subject_images_names = os.listdir(subject_dir_path)\n",
    "        \n",
    "        #------STEP-3--------\n",
    "        #go through each image name, read image, \n",
    "        #detect face and add face to list of faces\n",
    "        for image_name in subject_images_names:\n",
    "            \n",
    "            #ignore system files like .DS_Store\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue;\n",
    "            \n",
    "            #build image path\n",
    "            #sample image path = training-data/s1/1.pgm\n",
    "            image_path = subject_dir_path + \"/\" + image_name\n",
    "\n",
    "            #read image\n",
    "            #temp = cv2.imread(image_path)\n",
    "           # temp = Image.open(image_path)\n",
    "            img = io.imread(image_path)\n",
    "            # imgGray = color.rgb2gray(img)\n",
    "            #temp=Image.open('a.jpg') \n",
    "            img = img.astype('float32') / 255.0\n",
    "           \n",
    "                     \n",
    "            #------STEP-4--------\n",
    "            #for the purpose of this tutorial\n",
    "            #we will ignore faces that are not detected\n",
    "            if img is not None:\n",
    "                #add face to list of faces\n",
    "                \n",
    "                faces.append(img)\n",
    "#                 print(img.shape)\n",
    "#                 print(label)\n",
    "                #add label for this face\n",
    "                labels.append(label)\n",
    "              \n",
    "    \n",
    "    \n",
    "    # print(labels)\n",
    "\n",
    "    return faces, labels\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1cf099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared\n",
      "Data prepared\n",
      "Data prepared\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data...\")\n",
    "facestr, labelstr = prepare_training_data('C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\Datasets\\\\Faces\\\\training')\n",
    "print(\"Data prepared\")\n",
    "\n",
    "faceste, labelste = prepare_training_data('C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\Datasets\\\\Faces\\\\testing')\n",
    "print(\"Data prepared\")\n",
    "facesval, labelsval = prepare_training_data('C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\Datasets\\\\Faces\\\\validation')\n",
    "print(\"Data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c340bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "train pics:  (280, 80, 70)\n",
      "trainlabels:  (280,)\n",
      "test pics:  (80, 80, 70)\n",
      "test labels:  (80,)\n",
      "test pics:  (40, 80, 70)\n",
      "test labels:  (40,)\n"
     ]
    }
   ],
   "source": [
    "x_train=np.array(facestr)\n",
    "y_train=np.array(labelstr)\n",
    "print(x_train.ndim)\n",
    "print(\"train pics: \", x_train.shape)\n",
    "print(\"trainlabels: \", y_train.shape)\n",
    "x_test=np.array(faceste)\n",
    "y_test=np.array(labelste)\n",
    "print(\"test pics: \", x_test.shape)\n",
    "print(\"test labels: \", y_test.shape)\n",
    "x_val=np.array(facesval)\n",
    "y_val=np.array(labelsval)\n",
    "print(\"test pics: \", x_val.shape)\n",
    "print(\"test labels: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_names = []\n",
    "list_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7ff40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (280, 80, 70, 1)\n",
      "x_test:  (80, 80, 70, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1,80,70,1)\n",
    "x_test = x_test.reshape(-1,80,70,1)\n",
    "x_val = x_val.reshape(-1,80,70,1)\n",
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"x_test: \",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f7ec405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "num_classes = max(max(y_train), max(y_test), max(y_val))+1\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de818743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_ shape:  (280, 41)\n",
      "y_test_ shape:  (80, 41)\n",
      "[ 1  1  1  1  1  1  1 10 10 10 10 10 10 10 11 11 11 11 11 11 11 12 12 12\n",
      " 12 12 12 12 13 13 13 13 13 13 13 14 14 14 14 14 14 14 15 15 15 15 15 15\n",
      " 15 16 16 16 16 16 16 16 17 17 17 17 17 17 17 18 18 18 18 18 18 18 19 19\n",
      " 19 19 19 19 19  2  2  2  2  2  2  2 20 20 20 20 20 20 20 21 21 21 21 21\n",
      " 21 21 22 22 22 22 22 22 22 23 23 23 23 23 23 23 24 24 24 24 24 24 24 25\n",
      " 25 25 25 25 25 25 26 26 26 26 26 26 26 27 27 27 27 27 27 27 28 28 28 28\n",
      " 28 28 28 29 29 29 29 29 29 29  3  3  3  3  3  3  3 30 30 30 30 30 30 30\n",
      " 31 31 31 31 31 31 31 32 32 32 32 32 32 32 33 33 33 33 33 33 33 34 34 34\n",
      " 34 34 34 34 35 35 35 35 35 35 35 36 36 36 36 36 36 36 37 37 37 37 37 37\n",
      " 37 38 38 38 38 38 38 38 39 39 39 39 39 39 39  4  4  4  4  4  4  4 40 40\n",
      " 40 40 40 40 40  5  5  5  5  5  5  5  6  6  6  6  6  6  6  7  7  7  7  7\n",
      "  7  7  8  8  8  8  8  8  8  9  9  9  9  9  9  9]\n",
      "test lables\n",
      "[ 1  1 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19  2  2\n",
      " 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29  3  3 30 30\n",
      " 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39  4  4 40 40  5  5\n",
      "  6  6  7  7  8  8  9  9]\n",
      "[ 1 10 11 12 13 14 15 16 17 18 19  2 20 21 22 23 24 25 26 27 28 29  3 30\n",
      " 31 32 33 34 35 36 37 38 39  4 40  5  6  7  8  9]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "y_train_ = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_ = to_categorical(y_test, num_classes=num_classes)\n",
    "y_val_ = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "\n",
    "print(\"y_train_ shape: \",y_train_.shape)\n",
    "print(\"y_test_ shape: \",y_test_.shape)\n",
    "\n",
    "print(y_train)\n",
    "print('test lables')\n",
    "print(y_test)\n",
    "print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfb2b294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 - 4s - loss: 4.1370 - accuracy: 0.0117 - val_loss: 3.7128 - val_accuracy: 0.0250 - 4s/epoch - 2s/step\n",
      "Epoch 2/50\n",
      "2/2 - 3s - loss: 3.7284 - accuracy: 0.0273 - val_loss: 3.7117 - val_accuracy: 0.0500 - 3s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "2/2 - 2s - loss: 3.7070 - accuracy: 0.0329 - val_loss: 3.7079 - val_accuracy: 0.0250 - 2s/epoch - 798ms/step\n",
      "Epoch 4/50\n",
      "2/2 - 2s - loss: 3.7137 - accuracy: 0.0197 - val_loss: 3.7062 - val_accuracy: 0.0250 - 2s/epoch - 795ms/step\n",
      "Epoch 5/50\n",
      "2/2 - 2s - loss: 3.7425 - accuracy: 0.0197 - val_loss: 3.7048 - val_accuracy: 0.0250 - 2s/epoch - 799ms/step\n",
      "Epoch 6/50\n",
      "2/2 - 3s - loss: 3.6986 - accuracy: 0.0195 - val_loss: 3.6989 - val_accuracy: 0.0250 - 3s/epoch - 1s/step\n",
      "Epoch 7/50\n",
      "2/2 - 2s - loss: 3.6947 - accuracy: 0.0263 - val_loss: 3.6957 - val_accuracy: 0.0250 - 2s/epoch - 840ms/step\n",
      "Epoch 8/50\n",
      "2/2 - 3s - loss: 3.7195 - accuracy: 0.0117 - val_loss: 3.6956 - val_accuracy: 0.0250 - 3s/epoch - 1s/step\n",
      "Epoch 9/50\n",
      "2/2 - 2s - loss: 3.7030 - accuracy: 0.0197 - val_loss: 3.6934 - val_accuracy: 0.0250 - 2s/epoch - 816ms/step\n",
      "Epoch 10/50\n",
      "2/2 - 3s - loss: 3.6935 - accuracy: 0.0273 - val_loss: 3.6709 - val_accuracy: 0.0250 - 3s/epoch - 1s/step\n",
      "Epoch 11/50\n",
      "2/2 - 4s - loss: 3.6716 - accuracy: 0.0430 - val_loss: 3.6320 - val_accuracy: 0.0250 - 4s/epoch - 2s/step\n",
      "Epoch 12/50\n",
      "2/2 - 2s - loss: 3.6711 - accuracy: 0.0592 - val_loss: 3.6396 - val_accuracy: 0.1000 - 2s/epoch - 1s/step\n",
      "Epoch 13/50\n",
      "2/2 - 3s - loss: 3.8143 - accuracy: 0.0586 - val_loss: 3.6842 - val_accuracy: 0.1000 - 3s/epoch - 1s/step\n",
      "Epoch 14/50\n",
      "2/2 - 2s - loss: 3.6647 - accuracy: 0.0724 - val_loss: 3.6222 - val_accuracy: 0.0750 - 2s/epoch - 922ms/step\n",
      "Epoch 15/50\n",
      "2/2 - 2s - loss: 3.6128 - accuracy: 0.0789 - val_loss: 3.5738 - val_accuracy: 0.0250 - 2s/epoch - 932ms/step\n",
      "Epoch 16/50\n",
      "2/2 - 3s - loss: 3.5605 - accuracy: 0.0508 - val_loss: 3.4578 - val_accuracy: 0.0750 - 3s/epoch - 1s/step\n",
      "Epoch 17/50\n",
      "2/2 - 2s - loss: 3.5290 - accuracy: 0.0592 - val_loss: 3.5951 - val_accuracy: 0.0500 - 2s/epoch - 940ms/step\n",
      "Epoch 18/50\n",
      "2/2 - 2s - loss: 3.5381 - accuracy: 0.0592 - val_loss: 3.4405 - val_accuracy: 0.1000 - 2s/epoch - 914ms/step\n",
      "Epoch 19/50\n",
      "2/2 - 2s - loss: 3.3604 - accuracy: 0.1053 - val_loss: 3.0705 - val_accuracy: 0.3500 - 2s/epoch - 936ms/step\n",
      "Epoch 20/50\n",
      "2/2 - 2s - loss: 8.5447 - accuracy: 0.0592 - val_loss: 3.6424 - val_accuracy: 0.0500 - 2s/epoch - 1s/step\n",
      "Epoch 21/50\n",
      "2/2 - 2s - loss: 3.6343 - accuracy: 0.0921 - val_loss: 3.3530 - val_accuracy: 0.1750 - 2s/epoch - 968ms/step\n",
      "Epoch 22/50\n",
      "2/2 - 2s - loss: 3.3052 - accuracy: 0.1316 - val_loss: 3.1544 - val_accuracy: 0.2750 - 2s/epoch - 936ms/step\n",
      "Epoch 23/50\n",
      "2/2 - 2s - loss: 3.2574 - accuracy: 0.1250 - val_loss: 2.9961 - val_accuracy: 0.3250 - 2s/epoch - 917ms/step\n",
      "Epoch 24/50\n",
      "2/2 - 3s - loss: 3.0585 - accuracy: 0.1875 - val_loss: 2.7615 - val_accuracy: 0.4500 - 3s/epoch - 1s/step\n",
      "Epoch 25/50\n",
      "2/2 - 2s - loss: 2.7447 - accuracy: 0.2500 - val_loss: 2.9091 - val_accuracy: 0.1500 - 2s/epoch - 969ms/step\n",
      "Epoch 26/50\n",
      "2/2 - 3s - loss: 3.1967 - accuracy: 0.1484 - val_loss: 3.1176 - val_accuracy: 0.1250 - 3s/epoch - 1s/step\n",
      "Epoch 27/50\n",
      "2/2 - 2s - loss: 2.8559 - accuracy: 0.2105 - val_loss: 2.3453 - val_accuracy: 0.4500 - 2s/epoch - 917ms/step\n",
      "Epoch 28/50\n",
      "2/2 - 2s - loss: 2.8444 - accuracy: 0.2237 - val_loss: 2.3259 - val_accuracy: 0.4500 - 2s/epoch - 931ms/step\n",
      "Epoch 29/50\n",
      "2/2 - 2s - loss: 2.3070 - accuracy: 0.4276 - val_loss: 2.4024 - val_accuracy: 0.2750 - 2s/epoch - 965ms/step\n",
      "Epoch 30/50\n",
      "2/2 - 2s - loss: 2.6228 - accuracy: 0.2105 - val_loss: 2.2421 - val_accuracy: 0.4000 - 2s/epoch - 956ms/step\n",
      "Epoch 31/50\n",
      "2/2 - 2s - loss: 2.4889 - accuracy: 0.2566 - val_loss: 2.5200 - val_accuracy: 0.3250 - 2s/epoch - 988ms/step\n",
      "Epoch 32/50\n",
      "2/2 - 2s - loss: 2.5690 - accuracy: 0.2763 - val_loss: 2.0594 - val_accuracy: 0.4500 - 2s/epoch - 1s/step\n",
      "Epoch 33/50\n",
      "2/2 - 2s - loss: 2.5418 - accuracy: 0.2500 - val_loss: 1.8363 - val_accuracy: 0.5500 - 2s/epoch - 969ms/step\n",
      "Epoch 34/50\n",
      "2/2 - 2s - loss: 2.0781 - accuracy: 0.4145 - val_loss: 1.4841 - val_accuracy: 0.5750 - 2s/epoch - 994ms/step\n",
      "Epoch 35/50\n",
      "2/2 - 3s - loss: 1.7267 - accuracy: 0.5156 - val_loss: 1.0866 - val_accuracy: 0.7500 - 3s/epoch - 2s/step\n",
      "Epoch 36/50\n",
      "2/2 - 2s - loss: 1.5748 - accuracy: 0.5263 - val_loss: 1.3013 - val_accuracy: 0.7250 - 2s/epoch - 1s/step\n",
      "Epoch 37/50\n",
      "2/2 - 2s - loss: 1.7866 - accuracy: 0.4737 - val_loss: 1.5317 - val_accuracy: 0.5750 - 2s/epoch - 1s/step\n",
      "Epoch 38/50\n",
      "2/2 - 3s - loss: 1.9814 - accuracy: 0.4258 - val_loss: 1.1933 - val_accuracy: 0.7250 - 3s/epoch - 2s/step\n",
      "Epoch 39/50\n",
      "2/2 - 2s - loss: 1.2477 - accuracy: 0.6908 - val_loss: 0.9874 - val_accuracy: 0.7250 - 2s/epoch - 1s/step\n",
      "Epoch 40/50\n",
      "2/2 - 2s - loss: 2.0286 - accuracy: 0.4539 - val_loss: 0.9248 - val_accuracy: 0.7250 - 2s/epoch - 1s/step\n",
      "Epoch 41/50\n",
      "2/2 - 2s - loss: 1.2263 - accuracy: 0.6447 - val_loss: 0.6326 - val_accuracy: 0.8250 - 2s/epoch - 1s/step\n",
      "Epoch 42/50\n",
      "2/2 - 3s - loss: 0.7547 - accuracy: 0.7566 - val_loss: 0.6358 - val_accuracy: 0.8500 - 3s/epoch - 1s/step\n",
      "Epoch 43/50\n",
      "2/2 - 2s - loss: 1.4579 - accuracy: 0.5329 - val_loss: 1.1741 - val_accuracy: 0.7250 - 2s/epoch - 1s/step\n",
      "Epoch 44/50\n",
      "2/2 - 4s - loss: 1.2580 - accuracy: 0.6406 - val_loss: 0.5437 - val_accuracy: 0.8750 - 4s/epoch - 2s/step\n",
      "Epoch 45/50\n",
      "2/2 - 2s - loss: 0.6453 - accuracy: 0.8553 - val_loss: 0.5316 - val_accuracy: 0.8500 - 2s/epoch - 1s/step\n",
      "Epoch 46/50\n",
      "2/2 - 2s - loss: 0.7524 - accuracy: 0.8026 - val_loss: 0.3883 - val_accuracy: 0.9000 - 2s/epoch - 1s/step\n",
      "Epoch 47/50\n",
      "2/2 - 2s - loss: 0.4622 - accuracy: 0.8487 - val_loss: 0.2976 - val_accuracy: 0.9500 - 2s/epoch - 1s/step\n",
      "Epoch 48/50\n",
      "2/2 - 2s - loss: 0.5120 - accuracy: 0.8553 - val_loss: 0.4664 - val_accuracy: 0.8500 - 2s/epoch - 1s/step\n",
      "Epoch 49/50\n",
      "2/2 - 2s - loss: 0.6508 - accuracy: 0.7697 - val_loss: 0.4752 - val_accuracy: 0.8500 - 2s/epoch - 1s/step\n",
      "Epoch 50/50\n",
      "2/2 - 3s - loss: 0.4830 - accuracy: 0.8398 - val_loss: 0.3192 - val_accuracy: 0.9000 - 3s/epoch - 2s/step\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "face_model = Sequential()\n",
    "face_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(80, 70, 1)))\n",
    "face_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "face_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "face_model.add(Dropout(0.25))\n",
    "\n",
    "face_model.add(Flatten())\n",
    "face_model.add(Dense(512, activation='relu'))\n",
    "face_model.add(Dropout(0.5))\n",
    "face_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08)\n",
    "face_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "output_dir = \"C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\checkpoint\"\n",
    "csv_logger = CSVLogger(os.path.join(output_dir, 'training.log'))\n",
    "\n",
    "checkpoint_filepath_val = os.path.join(output_dir, 'model_chkpt_val.h5')\n",
    "model_checkpoint_val = ModelCheckpoint(filepath=checkpoint_filepath_val,\n",
    "                                       save_weights_only=False,\n",
    "                                       monitor='val_accuracy',\n",
    "                                       mode='max',\n",
    "                                       save_best_only=True,\n",
    "                                       verbose=1)\n",
    "\n",
    "checkpoint_filepath_acc = os.path.join(output_dir, 'model_chkpt_acc.h5')\n",
    "model_checkpoint_acc = ModelCheckpoint(filepath=checkpoint_filepath_acc,\n",
    "                                       save_weights_only=False,\n",
    "                                       monitor='accuracy',\n",
    "                                       mode='max',\n",
    "                                       save_best_only=True,\n",
    "                                       verbose=1)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    factor=0.7,\n",
    "    min_lr=0.00000000001\n",
    ")\n",
    "\n",
    "epoch = 50\n",
    "batch_size = 128\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.05, # Randomly zoom image \n",
    "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "history = face_model.fit(\n",
    "    datagen.flow(x_train, y_train_, batch_size=batch_size),\n",
    "    epochs=epoch,\n",
    "    validation_data=(x_val, y_val_),\n",
    "    verbose=2,\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "#     callbacks=[learning_rate_reduction,csv_logger, model_checkpoint_val, model_checkpoint_acc]\n",
    ")\n",
    "\n",
    "model_json = face_model.to_json()\n",
    "with open(\"face_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "face_model.save_weights('face_model.h5')\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe649828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "model_json = face_model.to_json()\n",
    "with open(\"face_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "face_model.save_weights('face_model.h5')\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82c847f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 78, 68, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 76, 66, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 38, 33, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 38, 33, 64)        0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 36, 31, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 34, 29, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 17, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 17, 14, 128)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 30464)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               15598080  \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 41)                21033     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15859369 (60.50 MB)\n",
      "Trainable params: 15859369 (60.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "face_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a828f1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate in test data\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3724 - accuracy: 0.9125\n",
      "test loss,test acc [0.3723891079425812, 0.9125000238418579]\n"
     ]
    }
   ],
   "source": [
    "print(\"evaluate in test data\")\n",
    "result=face_model.evaluate(x_test,y_test_,batch_size=64)\n",
    "print(\"test loss,test acc\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d882874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 58ms/step\n",
      "Cnn_accuracy is % 91.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_names=[]\n",
    "#y_pred = model.predict_classes(x_test)\n",
    "y_pred = np.argmax(face_model.predict(x_test), axis=-1)\n",
    "\n",
    "y_test = y_test.reshape(-1,)\n",
    "\n",
    "diff = y_test - y_pred\n",
    "diff = diff.reshape(-1,1)\n",
    "\n",
    "true = 0\n",
    "for i in range(0,len(diff)):\n",
    "    if diff[i] == 0:\n",
    "        true = true + 1\n",
    "\n",
    "Cnn_accuracy = round(100*true/len(diff),2)\n",
    "\n",
    "print(\"Cnn_accuracy is %\", Cnn_accuracy)\n",
    "\n",
    "list_names.append(\"CNN\")\n",
    "list_accuracy.append(Cnn_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "064ccb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.50      0.67         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      0.50      0.67         2\n",
      "           6       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      0.50      0.67         2\n",
      "          11       1.00      1.00      1.00         2\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       0.67      1.00      0.80         2\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       0.67      1.00      0.80         2\n",
      "          17       1.00      0.50      0.67         2\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      1.00      1.00         2\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       1.00      0.50      0.67         2\n",
      "          22       1.00      1.00      1.00         2\n",
      "          23       0.50      1.00      0.67         2\n",
      "          24       1.00      1.00      1.00         2\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         2\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       1.00      1.00      1.00         2\n",
      "          30       1.00      0.50      0.67         2\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       1.00      0.50      0.67         2\n",
      "          36       0.67      1.00      0.80         2\n",
      "          37       1.00      1.00      1.00         2\n",
      "          38       1.00      1.00      1.00         2\n",
      "          39       1.00      1.00      1.00         2\n",
      "          40       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.91        80\n",
      "   macro avg       0.95      0.91      0.91        80\n",
      "weighted avg       0.95      0.91      0.91        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29618de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming your model is named 'face_model'\n",
    "# predict = face_model.predict(x_test)\n",
    "# y_pred = np.argmax(predict, axis=-1)\n",
    "\n",
    "# y_test = y_test.reshape(-1,)\n",
    "\n",
    "# diff = y_test - y_pred\n",
    "# diff = diff.reshape(-1, 1)\n",
    "\n",
    "# true = 0\n",
    "# precisions = []\n",
    "\n",
    "# for i in range(0, len(diff)):\n",
    "#     if diff[i] == 0:\n",
    "#         true = true + 1\n",
    "#     # Calculate precision for the specific image\n",
    "#     precision = precision_score(y_test, y_pred, average='macro')\n",
    "#     precisions.append(precision)\n",
    "\n",
    "# cnn_accuracy = round(100 * true / len(diff), 2)\n",
    "\n",
    "# print(\"CNN accuracy is %\", cnn_accuracy)\n",
    "\n",
    "# list_names.append(\"CNN\")\n",
    "# list_accuracy.append(cnn_accuracy)\n",
    "\n",
    "# # Plot random images with true and predicted labels\n",
    "# plt.figure(figsize=(80, 100))\n",
    "# for i in range(40):  # Change the range based on the number of images you want to visualize\n",
    "#     index = random.randint(0, len(x_test) - 1)\n",
    "#     image = x_test[index].reshape(80, 70)  # Assuming grayscale images\n",
    "#     true_label = y_test[index]\n",
    "#     predicted_label = y_pred[index]\n",
    "\n",
    "#     # Check if the prediction is correct\n",
    "#     if true_label == predicted_label:\n",
    "#         color = 'green'\n",
    "#         accuracy_label = f'Correct\\n'\n",
    "#     else:\n",
    "#         color = 'red'\n",
    "#         accuracy_label = f'Incorrect\\n'\n",
    "#     accuracy_label_text = f'Test Accuracy: {cnn_accuracy}%\\n'\n",
    "#     precision_text = f'Precision: {precisions[index]:.2f}\\n'\n",
    "\n",
    "#     plt.subplot(8, 5, i + 1)\n",
    "#     plt.imshow(image, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     # Print labels below each image\n",
    "#     plt.text(0.5, -0.5, f'True: {true_label}\\nPredicted: {predicted_label}\\n{accuracy_label}\\n{accuracy_label_text}\\n{precision_text}', color=color, fontsize=50, horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# # Increase the spacing between subplots (both horizontally and vertically)\n",
    "# plt.subplots_adjust(wspace=0.4, hspace=0.8)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d4cf12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "[[5.2430250e-06 3.4496931e-03 2.9988079e-03 1.3571486e-02 9.1347471e-03\n",
      "  1.3409994e-02 2.2028308e-02 6.6727716e-03 2.9720394e-03 2.7042360e-03\n",
      "  1.5409944e-02 2.5389681e-03 7.3620264e-04 1.6378105e-03 4.0347117e-04\n",
      "  1.0787424e-02 4.7559354e-02 1.2591376e-02 4.6932586e-03 9.4502798e-04\n",
      "  5.9168179e-02 3.7174169e-03 1.3410048e-02 1.7604882e-01 5.6057656e-03\n",
      "  1.1010218e-02 2.6825248e-04 3.3871476e-03 1.4109131e-03 2.1517058e-01\n",
      "  1.2510069e-01 8.3801404e-02 3.7546240e-02 2.1192124e-02 1.2008377e-02\n",
      "  5.7687890e-03 7.9974849e-03 7.3085254e-04 3.3432685e-03 2.7271939e-02\n",
      "  1.1791501e-02]]\n",
      "[ 1  1 10 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19  2  2\n",
      " 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29  3  3 30 30\n",
      " 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39  4  4 40 40  5  5\n",
      "  6  6  7  7  8  8  9  9]\n",
      "Predicted Label : 22\n",
      "Predicted acc : 21.517057716846466\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def load_face_model(model_path):\n",
    "    return load_model(model_path)\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize the image to match the model's expected input shape (80, 70)\n",
    "    img_resized = cv2.resize(img_gray, (80, 70))\n",
    "    \n",
    "    # Expand dimensions to match the model's input shape\n",
    "    img_array = np.expand_dims(img_resized, axis=0)\n",
    "    img_array = np.expand_dims(img_array, axis=-1)\n",
    "    \n",
    "    # Normalize the pixel values to be between 0 and 1\n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def recognize_face_single_image(model, img_array):\n",
    "    # Predict the face using the trained model\n",
    "    face_prediction = model.predict(img_array)\n",
    "    # Get the predicted label index\n",
    "    print(face_prediction)\n",
    "    predicted_label_index = np.argmax(face_prediction[0])\n",
    "    predicted_label=y_test[predicted_label_index]\n",
    "    acc=face_prediction[0][predicted_label_index]*100\n",
    "    return predicted_label,acc\n",
    "\n",
    "# # Load the trained face recognition model\n",
    "# model_path = 'path/to/your/trained_model.h5'\n",
    "# face_model = load_face_model(model_path)\n",
    "\n",
    "# Provide the path to the image you want to test\n",
    "image_path = r\"C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\Datasets\\\\Faces\\\\testing\\\\s24\\\\231_24.jpg\"\n",
    "# image_path = r\"D:\\\\Downloads\\\\larine.jpg\"\n",
    "\n",
    "# Preprocess the image\n",
    "image_array = preprocess_image(image_path)\n",
    "\n",
    "# Recognize the face in the single image\n",
    "predicted_label,acc = recognize_face_single_image(face_model, image_array)\n",
    "print(y_test)\n",
    "# Print the predicted label index\n",
    "print(\"Predicted Label :\",predicted_label)\n",
    "print(\"Predicted acc :\",acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f382c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "larine.jpg\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[0.00000000e+00 7.90133903e-10 2.27737498e-32 1.51735284e-26\n",
      "  3.06580049e-28 2.52902467e-29 5.17768737e-24 1.25428895e-17\n",
      "  2.00088431e-20 0.00000000e+00 1.71243102e-30 1.08197593e-29\n",
      "  5.95248691e-23 1.64259882e-24 2.68269388e-27 9.87662420e-18\n",
      "  4.63877493e-30 1.25943702e-19 1.05980570e-16 1.60764780e-32\n",
      "  1.66483103e-37 1.00402445e-23 9.00291422e-36 6.46831183e-25\n",
      "  1.81494515e-27 0.00000000e+00 3.52664865e-13 2.05269287e-33\n",
      "  7.41355717e-01 5.31310883e-30 3.47352004e-26 1.50927552e-20\n",
      "  3.24233071e-07 2.34245674e-14 5.68684084e-23 2.58643955e-01\n",
      "  1.91350365e-32 6.53406761e-29 0.00000000e+00 7.53340897e-23\n",
      "  4.50051967e-21]]\n",
      "28\n",
      "22\n",
      "[32  1 20 10 11 11 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19  2  2\n",
      " 20 20 21 21 22 22 23 23 24 24 25 25 15 12 27 27 28 28 29 29  3  3 30 38\n",
      " 31 31 32 32 33 33 34 34 35 35 36 36 37 37 38 38 39 39  4  4 40 17  5 12\n",
      "  6  6  7  7  8  8  9  9]\n",
      "[ 1 10 11 12 13 14 15 16 17 18 19  2 20 21 22 23 24 25 26 27 28 29  3 30\n",
      " 31 32 33 34 35 36 37 38 39  4 40  5  6  7  8  9]\n",
      "Prediction Label:  28\n",
      "Prediction Accuracy: 74.14%\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "# # Load JSON data from the file\n",
    "# with open(\"labels_model.json\", \"r\") as json_file:\n",
    "#     labels_json = json_file.read()\n",
    "\n",
    "# # Deserialize JSON to a dictionary\n",
    "# class_indices_dict = json.loads(labels_json)\n",
    "\n",
    "# # Convert dictionary values to an array\n",
    "# class_indices_array = list(class_indices_dict)\n",
    "# # Load the model structure from the saved json file\n",
    "# with open(\"C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\face_model.json\", \"r\") as json_file:\n",
    "#     loaded_model_json = json_file.read()\n",
    "#     face_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the trained weights into the model\n",
    "# face_model.load_weights('C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\checkpoint\\\\model_chkpt_val.h5')\n",
    "# face_model.load_weights('face_model.h5')\n",
    "\n",
    "# Load the image for detection\n",
    "# img_path = r\"C:\\\\Users\\\\minnu\\\\Downloads\\\\Face-Recognition\\\\Face-Recognition-master\\\\Datasets\\\\Faces\\\\testing\\\\s18\\\\171_18.jpg\"\n",
    "img_path = r\"D:\\\\Downloads\\\\larine.jpg\"\n",
    "\n",
    "\n",
    "print(os.path.basename(img_path))\n",
    "\n",
    "img = image.load_img(img_path, target_size=(80, 70), color_mode=\"grayscale\")\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# print(img_array)\n",
    "# img_array /= 255.0\n",
    "\n",
    "# img_array = io.imread(image_path).reshape(-1,80,70,1)\n",
    "\n",
    "# Make prediction\n",
    "face_prediction = face_model.predict(img_array)\n",
    "print(face_prediction)\n",
    "\n",
    "\n",
    "# print(emotion_prediction[0][23]*100)\n",
    "\n",
    "# Get the predicted emotion label\n",
    "predicted_label_index = np.argmax(face_prediction[0])\n",
    "accuracy_value = float(face_prediction[0][predicted_label_index] * 100)\n",
    "print(predicted_label_index)\n",
    "print(y_pred[predicted_label_index])\n",
    "\n",
    "print(y_pred)\n",
    "print(y_val)\n",
    "# Display the image with the predicted label\n",
    "img = cv2.imread(img_path)\n",
    "cv2.putText(img, f\"Predicted: {predicted_label_index}\", (1, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "print(f\"Prediction Label: \",predicted_label_index)\n",
    "\n",
    "# Extract the numerical value of accuracy\n",
    "print(f\"Prediction Accuracy: {accuracy_value:.2f}%\")\n",
    "\n",
    "cv2.imshow('Image Detection', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae96b404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)\n",
    "len(face_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9900cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load the pre-trained face cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load your pre-trained face recognition model\n",
    "# (Assuming you have a model named 'face_model' loaded previously)\n",
    "\n",
    "# Open a video capture object (0 represents the default camera, you can change it to the file path if using a video file)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame using Haar Cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Resize the face region to the required input size for your face recognition model\n",
    "        face_roi_resized = cv2.resize(face_roi, (80, 70))\n",
    "\n",
    "        # Convert to a format suitable for prediction\n",
    "        img_array = image.img_to_array(face_roi_resized)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "        # Since the model expects grayscale images, discard color channels\n",
    "        img_array = np.mean(img_array, axis=-1, keepdims=True)\n",
    "\n",
    "        # Make predictions using your face recognition model\n",
    "        face_prediction = face_model.predict(img_array)\n",
    "\n",
    "        # Assuming face_model.predict returns a list of probabilities\n",
    "        predicted_label_index = np.argmax(face_prediction)\n",
    "        predicted_label = f\"Label: {predicted_label_index}\"\n",
    "\n",
    "        # Get the accuracy value for the predicted label\n",
    "        accuracy_value = float(face_prediction[0][predicted_label_index] * 100)\n",
    "        accuracy_label = f\"Accuracy: {accuracy_value:.2f}%\"\n",
    "\n",
    "        # If the accuracy is greater than 60, draw a rectangle around the face and display label and accuracy\n",
    "        if accuracy_value > 60:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, accuracy_label, (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e240721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
